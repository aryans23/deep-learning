\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Data representation using Word Embeddings}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Word2Vec}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{Role of Context Types and Dimentionality in Learning Word Embeddings}{section.2}% 4
\BOOKMARK [1][-]{section.3}{Advancement of Recurrent Neural Networks in NLP}{}% 5
\BOOKMARK [2][-]{subsection.3.1}{Sequence to Sequence Learning with Neural Networks}{section.3}% 6
\BOOKMARK [2][-]{subsection.3.2}{Multilingual Part-of-Speech Tagging with Bidirectional Long-Short-Term-Memory models with Auxiliary Loss}{section.3}% 7
\BOOKMARK [2][-]{subsection.3.3}{Aspect-Based Sentiment Analysis}{section.3}% 8
\BOOKMARK [1][-]{section.4}{Attention based Deep Learning Models for Natural Language Processing}{}% 9
\BOOKMARK [2][-]{subsection.4.1}{Neural Machine Translation by jointly learning to align and translate}{section.4}% 10
\BOOKMARK [2][-]{subsection.4.2}{Effective Approaches to Attention Based Neural Machine Translation}{section.4}% 11
\BOOKMARK [2][-]{subsection.4.3}{Text Understanding with the Attention Sum Reader Network}{section.4}% 12
\BOOKMARK [2][-]{subsection.4.4}{A Structured Self-Attentive Sentence Embedding}{section.4}% 13
\BOOKMARK [2][-]{subsection.4.5}{Google's Neural Machine Translation System}{section.4}% 14
\BOOKMARK [1][-]{section.5}{Conclusion}{}% 15
\BOOKMARK [1][-]{section*.25}{Appendices}{}% 16
\BOOKMARK [1][-]{Appendix.1.A}{Recurrent Neural Network}{}% 17
\BOOKMARK [1][-]{Appendix.1.B}{Vanishing Gradients and Explosion Problems}{}% 18
\BOOKMARK [1][-]{Appendix.1.C}{Gated Recurrent Units}{}% 19
\BOOKMARK [1][-]{Appendix.1.D}{Long Short Term Memory}{}% 20
