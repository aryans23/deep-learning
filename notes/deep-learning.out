\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Basics of Machine Learning}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Validation}{section.2}% 3
\BOOKMARK [1][-]{section.3}{Linear Classifiers}{}% 4
\BOOKMARK [2][-]{subsection.3.1}{Multiclass SVM loss}{section.3}% 5
\BOOKMARK [2][-]{subsection.3.2}{Softmax Classifiers}{section.3}% 6
\BOOKMARK [1][-]{section.4}{Neurons}{}% 7
\BOOKMARK [2][-]{subsection.4.1}{Backpropagation}{section.4}% 8
\BOOKMARK [3][-]{subsubsection.4.1.1}{Practical Aspects}{subsection.4.1}% 9
\BOOKMARK [2][-]{subsection.4.2}{Single neuron as a classifier}{section.4}% 10
\BOOKMARK [3][-]{subsubsection.4.2.1}{Sigmoid}{subsection.4.2}% 11
\BOOKMARK [3][-]{subsubsection.4.2.2}{tanh}{subsection.4.2}% 12
\BOOKMARK [3][-]{subsubsection.4.2.3}{ReLU}{subsection.4.2}% 13
\BOOKMARK [3][-]{subsubsection.4.2.4}{Leaky ReLU}{subsection.4.2}% 14
\BOOKMARK [3][-]{subsubsection.4.2.5}{Maxout}{subsection.4.2}% 15
\BOOKMARK [1][-]{section.5}{Neural Networks}{}% 16
\BOOKMARK [2][-]{subsection.5.1}{Data Preprocessing}{section.5}% 17
\BOOKMARK [3][-]{subsubsection.5.1.1}{Mean substraction}{subsection.5.1}% 18
\BOOKMARK [3][-]{subsubsection.5.1.2}{Normalization}{subsection.5.1}% 19
\BOOKMARK [3][-]{subsubsection.5.1.3}{PCA}{subsection.5.1}% 20
\BOOKMARK [2][-]{subsection.5.2}{Weight Initialization}{section.5}% 21
\BOOKMARK [3][-]{subsubsection.5.2.1}{Pitfall: All zero initialization}{subsection.5.2}% 22
\BOOKMARK [3][-]{subsubsection.5.2.2}{Small random numbers}{subsection.5.2}% 23
\BOOKMARK [2][-]{subsection.5.3}{Things to monitor while learning}{section.5}% 24
\BOOKMARK [3][-]{subsubsection.5.3.1}{Loss Function}{subsection.5.3}% 25
\BOOKMARK [3][-]{subsubsection.5.3.2}{Train/Val accuracy}{subsection.5.3}% 26
\BOOKMARK [2][-]{subsection.5.4}{Parameter Updates}{section.5}% 27
\BOOKMARK [3][-]{subsubsection.5.4.1}{Vanilla SGD}{subsection.5.4}% 28
\BOOKMARK [3][-]{subsubsection.5.4.2}{Momentum Update SGD}{subsection.5.4}% 29
\BOOKMARK [3][-]{subsubsection.5.4.3}{Nesterov Momentum SGD}{subsection.5.4}% 30
\BOOKMARK [3][-]{subsubsection.5.4.4}{Annealing the learning rate over time}{subsection.5.4}% 31
\BOOKMARK [3][-]{subsubsection.5.4.5}{Second Order Updates}{subsection.5.4}% 32
\BOOKMARK [3][-]{subsubsection.5.4.6}{Adagrad}{subsection.5.4}% 33
\BOOKMARK [3][-]{subsubsection.5.4.7}{RMSprop}{subsection.5.4}% 34
\BOOKMARK [3][-]{subsubsection.5.4.8}{Adam}{subsection.5.4}% 35
\BOOKMARK [2][-]{subsection.5.5}{Model Ensemble}{section.5}% 36
\BOOKMARK [2][-]{subsection.5.6}{Transfer Learning}{section.5}% 37
\BOOKMARK [1][-]{section.6}{Convolutional Neural Networks}{}% 38
\BOOKMARK [2][-]{subsection.6.1}{Architecture Overview}{section.6}% 39
\BOOKMARK [2][-]{subsection.6.2}{Layers used to build ConvNets}{section.6}% 40
\BOOKMARK [3][-]{subsubsection.6.2.1}{Convolutional Layer}{subsection.6.2}% 41
\BOOKMARK [3][-]{subsubsection.6.2.2}{Pooling Layer}{subsection.6.2}% 42
\BOOKMARK [3][-]{subsubsection.6.2.3}{Fully Connected Layer}{subsection.6.2}% 43
\BOOKMARK [1][-]{section*.2}{Appendices}{}% 44
\BOOKMARK [1][-]{Appendix.1.A}{A simple neural network model for recognizing digits on MNIST data}{}% 45
