\select@language {english}
\contentsline {section}{\numberline {1}Introduction}{3}{section.1}
\contentsline {section}{\numberline {2}Basics of Machine Learning}{3}{section.2}
\contentsline {subsection}{\numberline {2.1}Validation}{3}{subsection.2.1}
\contentsline {section}{\numberline {3}Linear Classifiers}{3}{section.3}
\contentsline {subsection}{\numberline {3.1}Multiclass SVM loss}{3}{subsection.3.1}
\contentsline {subsection}{\numberline {3.2}Softmax Classifiers}{4}{subsection.3.2}
\contentsline {section}{\numberline {4}Neurons}{4}{section.4}
\contentsline {subsection}{\numberline {4.1}Backpropagation}{4}{subsection.4.1}
\contentsline {subsubsection}{\numberline {4.1.1}Practical Aspects}{4}{subsubsection.4.1.1}
\contentsline {subsection}{\numberline {4.2}Single neuron as a classifier}{4}{subsection.4.2}
\contentsline {subsubsection}{\numberline {4.2.1}Sigmoid}{5}{subsubsection.4.2.1}
\contentsline {subsubsection}{\numberline {4.2.2}tanh}{5}{subsubsection.4.2.2}
\contentsline {subsubsection}{\numberline {4.2.3}ReLU}{5}{subsubsection.4.2.3}
\contentsline {subsubsection}{\numberline {4.2.4}Leaky ReLU}{6}{subsubsection.4.2.4}
\contentsline {subsubsection}{\numberline {4.2.5}Maxout}{6}{subsubsection.4.2.5}
\contentsline {section}{\numberline {5}Neural Networks}{6}{section.5}
\contentsline {subsection}{\numberline {5.1}Data Preprocessing}{6}{subsection.5.1}
\contentsline {subsubsection}{\numberline {5.1.1}Mean substraction}{6}{subsubsection.5.1.1}
\contentsline {subsubsection}{\numberline {5.1.2}Normalization}{6}{subsubsection.5.1.2}
\contentsline {subsubsection}{\numberline {5.1.3}PCA}{7}{subsubsection.5.1.3}
\contentsline {subsection}{\numberline {5.2}Weight Initialization}{7}{subsection.5.2}
\contentsline {subsubsection}{\numberline {5.2.1}Pitfall: All zero initialization}{7}{subsubsection.5.2.1}
\contentsline {subsubsection}{\numberline {5.2.2}Small random numbers}{7}{subsubsection.5.2.2}
\contentsline {subsection}{\numberline {5.3}Things to monitor while learning}{8}{subsection.5.3}
\contentsline {subsubsection}{\numberline {5.3.1}Loss Function}{8}{subsubsection.5.3.1}
\contentsline {subsubsection}{\numberline {5.3.2}Train/Val accuracy}{8}{subsubsection.5.3.2}
\contentsline {subsection}{\numberline {5.4}Parameter Updates}{8}{subsection.5.4}
\contentsline {subsubsection}{\numberline {5.4.1}Vanilla SGD}{8}{subsubsection.5.4.1}
\contentsline {subsubsection}{\numberline {5.4.2}Momentum Update SGD}{9}{subsubsection.5.4.2}
\contentsline {subsubsection}{\numberline {5.4.3}Nesterov Momentum SGD}{9}{subsubsection.5.4.3}
\contentsline {subsubsection}{\numberline {5.4.4}Annealing the learning rate over time}{9}{subsubsection.5.4.4}
\contentsline {subsubsection}{\numberline {5.4.5}Second Order Updates}{10}{subsubsection.5.4.5}
\contentsline {subsubsection}{\numberline {5.4.6}Adagrad}{10}{subsubsection.5.4.6}
\contentsline {subsubsection}{\numberline {5.4.7}RMSprop}{10}{subsubsection.5.4.7}
\contentsline {subsubsection}{\numberline {5.4.8}Adam}{10}{subsubsection.5.4.8}
\contentsline {subsection}{\numberline {5.5}Model Ensemble}{11}{subsection.5.5}
\contentsline {subsection}{\numberline {5.6}Transfer Learning}{11}{subsection.5.6}
\contentsline {section}{\numberline {6}Convolutional Neural Networks}{11}{section.6}
\contentsline {subsection}{\numberline {6.1}Architecture Overview}{11}{subsection.6.1}
\contentsline {subsection}{\numberline {6.2}Layers used to build ConvNets}{12}{subsection.6.2}
\contentsline {subsubsection}{\numberline {6.2.1}Convolutional Layer}{13}{subsubsection.6.2.1}
\contentsline {subsubsection}{\numberline {6.2.2}Pooling Layer}{14}{subsubsection.6.2.2}
\contentsline {subsubsection}{\numberline {6.2.3}Fully Connected Layer}{15}{subsubsection.6.2.3}
\contentsline {section}{Appendices}{15}{section*.2}
\contentsline {section}{\numberline {A}A simple neural network model for recognizing digits on MNIST data}{15}{Appendix.1.A}
